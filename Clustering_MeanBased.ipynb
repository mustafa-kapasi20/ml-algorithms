{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering using Mean as a parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is the task of partitioning the dataset into groups, called clusters. The goal is to split up the data in such a way that points within a single cluster are very similar and points in different clusters are different. Similarly to classification algorithms, clustering algorithms assign (or predict) a number to each data point, indicating which cluster a particular point belongs to.\n",
    "\n",
    "The previous talk on SVM has been concerned with predicting the values of one or more outputs or response variables $Y = (Y_1, . . . , Y_m)$ for a given set of input or predictor variables $X^{T} = (X_1, . . . , X_p)$. Denoted by $x^T_i = (x_{i1}, . . . , x_{ip})$ the inputs for the $i^{th}$ training case, and let $y_{i}$ be a response measurement. The predictions are based on the training sample $(x_1, y_1), . . . , (x_N , y_N)$ of previously solved cases, where the joint values of all of the variables are known. This is called supervised learning or learning with a teacher. Under this metaphor the student presents an answer $\\hat{y}_i$ for each $x_i$ in the training sample, and the supervisor or teacher provides either the correct answer and/or an error associated with the student’s answer. This is usually characterized by some loss function $L(y, \\hat{y})$. For example:\n",
    "\n",
    "$$L(y, \\hat{y}) = (y - \\hat{y})^2$$\n",
    "\n",
    "If one supposes that $(X, Y)$ are random variables represented by some joint probability density $Pr(X, Y)$, then supervised learning can be formally characterized as a density estimation problem where one is concerned with determining properties of the conditional density $Pr(Y|X)$. Usually the properties of interest are the location parameters $\\mu$ that minimize the expected error at each x,\n",
    "\n",
    "$$\\mu(x) = \\text{argmin}_{\\theta} \\;E_{Y|X} L(Y, \\theta)$$\n",
    "\n",
    "\n",
    "In this talk we address unsupervised learning or learning without a teacher. In this case one has a set of $N$ observations $(x_1,x_2,...,x_N)$ of a random p-vector $X$ having joint density $Pr(X)$. The goal is to directly infer the properties of this probability density without the help of a supervisor or teacher providing correct answers or degree-of-error for each observation. The dimension of $X$ is sometimes much higher than in supervised learning, and the properties of interest are often more complicated than simple location estimates. These factors are somewhat mitigated by the fact that $X$ represents all of the variables under consideration; one is not required to infer how the properties of $Pr(X)$ change, conditioned on the changing values of another set of variables. With unsupervised learning we aim to find a plausible compact description of the data. An objective is used to quantify the accuracy of the description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import time\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "from IPython import display\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy as sp\n",
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Generating Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(prob, mean, cov, size):\n",
    "    if np.sum(prob) != 1.0:\n",
    "        prob = prob / np.sum(prob) # Normalizing probabilities\n",
    "    k = len(prob)\n",
    "    nj = np.random.multinomial(size, prob) # Generate sample sizes by probability distribution.\n",
    "    x = []\n",
    "    y = []\n",
    "    # generate multivariate normally distributed data points using mean, \n",
    "    # covariance and sample size as parameters \n",
    "    for j in range(k):\n",
    "        x.extend(np.random.multivariate_normal(mean[j], cov[j], size=int(nj[j]))) \n",
    "        y = np.append(y, np.ones(nj[j])*j).astype(int)\n",
    "    x = np.array(x)\n",
    "    order = np.arange(size) \n",
    "    np.random.shuffle(order)\n",
    "    return x[order], y[order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common clustering functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(x, y):\n",
    "    return np.linalg.norm(x-y)\n",
    "\n",
    "def group_datapoints(X, Y):\n",
    "    clusters = {}\n",
    "    for x,y in zip(X, Y):\n",
    "        if y not in clusters:\n",
    "            clusters[y] = []\n",
    "        clusters[y] += [x]\n",
    "\n",
    "    for i in clusters:\n",
    "        clusters[i] = np.array(clusters[i])\n",
    "    return clusters\n",
    "\n",
    "def calculate_centroids(clusters):\n",
    "    return {i: np.mean(clusters[i], axis=0) for i in clusters}\n",
    "\n",
    "def get_clusters(data, centroids):\n",
    "    eps = 10**5\n",
    "    old_centroids = centroids.copy()\n",
    "    fig = plt.figure(figsize = (10,8))\n",
    "    it = 0\n",
    "    while eps > 10**-5:\n",
    "        it += 1\n",
    "        clusters = {}\n",
    "        yhat = []\n",
    "        for x in data:\n",
    "            idx = np.argmin([euclidian_distance(x, old_centroids[i]) \n",
    "                             for i in old_centroids])\n",
    "            yhat += [idx]\n",
    "            \n",
    "        clusters = group_datapoints(data, yhat)\n",
    "        centroids = calculate_centroids(clusters)\n",
    "        visualize_kmeans(clusters, centroids)\n",
    "\n",
    "        eps = np.mean([euclidian_distance(centroids[i], old_centroids[i]) \n",
    "                      for i in clusters])\n",
    "        old_centroids = centroids.copy()\n",
    "        time.sleep(0.5)\n",
    "    print('Iterations: %d' % (it))\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_kmeans(clusters, centroids):\n",
    "    plt.clf()\n",
    "    for i in clusters:\n",
    "        plt.plot(clusters[i][:,0],clusters[i][:,1], 'o')\n",
    "    plt.plot(np.array(list(centroids.values()))[:,0], \n",
    "             np.array(list(centroids.values()))[:,1], \n",
    "             'yo', markersize=18)\n",
    "    plt.tight_layout()\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "def visualize_data(X, Y):\n",
    "    # segregate clusters based on generated data labels for visualization\n",
    "    clusters = group_datapoints(X, Y)\n",
    "    centroids = calculate_centroids(clusters)\n",
    "\n",
    "    # Visualize the simulated points\n",
    "    fig = plt.figure(figsize = (10,8))\n",
    "    plt.plot(X[:,0], X[:,1], 'o')\n",
    "    plt.tight_layout()\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    time.sleep(10)\n",
    "    \n",
    "    # Visualize the clusters\n",
    "    visualize_kmeans(clusters, centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Random Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pis = np.array([0.3, 0.3, 0.2, 0.2])\n",
    "mus = np.array([[0, 0], [3, 3], [0, 4], [3, -2]])\n",
    "sigmas = np.array([[[1.0, 0.3],[0.3,1.0]], \n",
    "                   np.diag((0.1,1.0)), \n",
    "                   np.diag((0.4,0.1)), \n",
    "                   np.diag((0.1,0.1))])\n",
    "size = 1000\n",
    "\n",
    "# np.random.seed(123)\n",
    "X, Y = generate_samples(pis, mus, sigmas, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_data(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-Means clustering is one of the simplest and most commonly used clustering algorithms. It tries to find cluster centers that are representative of certain regions of the data. The algorithm alternates between two steps: assigning each data point to the closest cluster center, and then setting each cluster center as the mean of the data points that are assigned to it. The algorithm is finished when the assignment of instances to clusters no longer changes.\n",
    "\n",
    "Expectation–maximization (E–M) is a powerful algorithm that comes up in a variety of contexts within data science. k-means is a particularly simple and easy-to-understand application of the algorithm, and we will walk through it briefly here. In short, the expectation–maximization approach here consists of the following procedure:\n",
    "\n",
    "1. Guess some cluster centers\n",
    "2. Repeat until converged\n",
    "3. E-Step: assign points to the nearest cluster center\n",
    "4. M-Step: set the cluster centers to the mean\n",
    "\n",
    "Here the \"E-step\" or \"Expectation step\" is so-named because it involves updating our expectation of which cluster each point belongs to. The \"M-step\" or \"Maximization step\" is so-named because it involves maximizing some fitness function that defines the location of the cluster centers—in this case, that maximization is accomplished by taking a simple mean of the data in each cluster.\n",
    "\n",
    "The literature about this algorithm is vast, but can be summarized as follows: under typical circumstances, each repetition of the E-step and M-step will always result in a better estimate of the cluster characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clusters_kmeans(X, k):\n",
    "    eps = 10**5\n",
    "    idx = np.random.choice(range(len(X)), size=k, replace=False)\n",
    "    old_centroids = {i:m for i,m in enumerate(X[idx])}\n",
    "    clusters = get_clusters(X, old_centroids)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specified that we are looking for four clusters, so the algorithm was initialized by declaring four data points as cluster centers. Then the iterative algorithm starts: Each data point is assigned to the cluster center it is closest to. Next, the cluster centers are updated to be the mean of the assigned points. Then the process is repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = find_clusters_kmeans(X, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meanshift Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanshift is a clustering algorithm that assigns the datapoints to the clusters iteratively by shifting points towards the high density regions. As such, it is also known as the mode-seeking algorithm. Meanshift algorithm has applications in the field of image processing and computer vision.\n",
    "\n",
    "Given a set of datapoints, the algorithm iteratively assign each datapoint towards the closest cluster centroid. The direction to the closest cluster centroid is determined by where most of the points nearby are at. So each iteration each data point will move closer to where the most points are at, which is or will lead to the cluster center. When the algorithm stops, each point is assigned to a cluster.\n",
    "\n",
    "Unlike the popular K-Means algorithm, meanshift does not require specifying the number of clusters in advance. The number of clusters is determined by the algorithm with respect to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a couple of helper functions. We will need a few things before we start to run Meanshift on a set of datapoints X:\n",
    "\n",
    "1. A function $N(x)$ to determine what are the neighbours of a point $x \\in X$. The neighbouring points are the points within a certain radius. The distance metric is usually Euclidean Distance.\n",
    "2. A kernel $K(d)$ to use in Meanshift. $K$ is usually a Gaussian or a Linear Kernel, and $d$ is the distance between two datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbourhood_points(X, x_centroid, radius = 5):\n",
    "    eligible_X = []\n",
    "    for x in X:\n",
    "        distance_between = euclidian_distance(x, x_centroid)\n",
    "        if distance_between <= radius:\n",
    "            eligible_X.append(x)\n",
    "    return eligible_X\n",
    "\n",
    "def gaussian_kernel(distance, kernel_bandwidth):\n",
    "    val = (1/(kernel_bandwidth*math.sqrt(2*math.pi))) * np.exp(-0.5*((distance / kernel_bandwidth))**2)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how the algorithm works:\n",
    "\n",
    "1. For each datapoint $x \\in X$, find the neighbouring points $N(x)$ of $x$.\n",
    "2. For each datapoint $x \\in X$, calculate the mean shift $m(x)$ from this equation: \n",
    "\n",
    "$$m(x) = \\frac{\\sum_{x_i \\in N(x_i)} K(x_i-x)x_i }{\\sum_{x_i \\in N(x_i)} K(x_i-x)} $$\n",
    "\n",
    "3. For each datapoint $x \\in X$, update $x \\leftarrow m(x)$.\n",
    "4. Repeat 1 for $N_{iteations}$ or until the points are stationary or almost stationary.\n",
    "\n",
    "For a Gaussian Kernel\n",
    "$$K(x_i -x) = \\frac{1}{\\sqrt{2\\pi w^2}}e^{-\\frac{1}{2}\\frac{(x_i-x)^T(x_i-x)}{w^2}}$$\n",
    "\n",
    "For a Linear Kernel\n",
    "$$K(x_i -x) = c $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mean_shift(data, centroids):\n",
    "    plt.plot(data[:,0], data[:,1], 'bo')\n",
    "    plt.plot(np.array(list(centroids.values()))[:,0], \n",
    "             np.array(list(centroids.values()))[:,1], 'ro')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "def optimize_centroid(centroid, neighbours, kernel_bandwidth=2, kernel='linear'):\n",
    "    numerator = 0\n",
    "    denominator = 0\n",
    "    for neighbour in neighbours:\n",
    "        distance = euclidian_distance(neighbour, centroid)\n",
    "        if kernel == 'linear':\n",
    "            weight = 1\n",
    "        elif kernel == 'gaussian':\n",
    "            weight = gaussian_kernel(distance, kernel_bandwidth=kernel_bandwidth)\n",
    "        numerator += (weight * neighbour)\n",
    "        denominator += weight\n",
    "    new_centroid = numerator / denominator\n",
    "    return new_centroid\n",
    "\n",
    "def optimize_all_centroids(data, centroids, kernel_bandwidth=2, kernel='linear'):\n",
    "    new_centroids = []\n",
    "    for i in centroids:\n",
    "        centroid = centroids[i]\n",
    "        neighbours = neighbourhood_points(data, centroid, radius=kernel_bandwidth)\n",
    "        new_centroid = optimize_centroid(centroid, neighbours, kernel=kernel,\n",
    "                                         kernel_bandwidth=kernel_bandwidth)\n",
    "        new_centroids.append(tuple(new_centroid))\n",
    "\n",
    "    uniques = np.array(sorted(list(set(new_centroids))))\n",
    "    centroids = {i:m for i,m in enumerate(uniques)}\n",
    "    return centroids\n",
    "\n",
    "def centroids_convergance(data, centroids, kernel_bandwidth=2, radius_check=True, \n",
    "                          kernel='linear', thresh=10**-3):\n",
    "    if radius_check: \n",
    "        it = 0\n",
    "    while True:\n",
    "        if radius_check:\n",
    "            it += 1\n",
    "            plt.clf()\n",
    "            plt.title('Iteration: %d Centroids: %d' % (it, len(centroids)))\n",
    "            visualize_mean_shift(data, centroids)\n",
    "        old_centroids = dict(centroids)\n",
    "        centroids = optimize_all_centroids(data, centroids, kernel=kernel,\n",
    "                                           kernel_bandwidth=kernel_bandwidth)\n",
    "\n",
    "        optimized = False\n",
    "        if len(centroids) == len(old_centroids):\n",
    "            eps = np.mean([euclidian_distance(centroids[i], old_centroids[i]) \n",
    "                           for i in centroids])\n",
    "            if eps < thresh:\n",
    "                optimized = True\n",
    "                \n",
    "        if optimized:\n",
    "            break\n",
    "            \n",
    "    if radius_check:\n",
    "        print('Iterations: %d' % (it))        \n",
    "    return centroids\n",
    "\n",
    "def find_clusters_meanshift(data, kernel_bandwidth=2, kernel='linear', \n",
    "                            sample_fraction=1, sleep_time=20, thresh=10**-3):\n",
    "    centroids = {}\n",
    "    start = time.time()\n",
    "    if sample_fraction == 1:\n",
    "        centroids = data.copy()\n",
    "    else:\n",
    "        idx = np.random.choice(range(len(data)), \n",
    "                               size=round(sample_fraction*len(data)), \n",
    "                               replace=False)\n",
    "        centroids = data[idx]\n",
    "    centroids = {i:m for i,m in enumerate(centroids)}\n",
    "    \n",
    "    radius_check = kernel_bandwidth > 0.1\n",
    "    if radius_check:\n",
    "        fig = plt.figure(figsize = (10,8))\n",
    "    centroids = centroids_convergance(data, centroids, kernel_bandwidth=kernel_bandwidth, \n",
    "                                      radius_check=radius_check, kernel=kernel, thresh=thresh)\n",
    "    \n",
    "    if radius_check:\n",
    "        n_centroids = len(centroids)\n",
    "        t_int = time.time() - start\n",
    "        print('Centroids detected: %d' % n_centroids)\n",
    "        print('Time: %0.2fs' % t_int)\n",
    "        visualize_mean_shift(np.array(data), centroids)\n",
    "        time.sleep(sleep_time)\n",
    "        centroids = find_clusters_meanshift(list(centroids.values()), kernel_bandwidth=0.1)\n",
    "        plt.clf()\n",
    "        visualize_mean_shift(np.array(data), centroids)\n",
    "        print('Initial Centroids detected: %d' % n_centroids)\n",
    "        print('Initial Optimization Time: %0.2fs' % t_int)\n",
    "        print('Optimized Centroids detected: %d' % len(centroids))\n",
    "        print('Total Optimization Time: %0.2fs' % (time.time()-start-sleep_time))\n",
    "        time.sleep(sleep_time)\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = find_clusters_meanshift(X, kernel_bandwidth=2.25, kernel='gaussian', \n",
    "                                    sample_fraction=0.15, sleep_time=2, thresh=10**-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refactoring using the same clustering routine as done for K-means Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = get_clusters(X, centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
